const fs = require("node:fs/promises");

(async()=>{
    const fileHandleRead = await fs.open("test.txt", "r")

    const stream = fileHandleRead.createReadStream()

    stream.on("data", (chunk)=>{
        console.log("------------")
        console.log(chunk)
    })
})()


output ------>>>>>

------------
<Buffer 20 30 20 20 31 20 20 32 20 20 33 20 20 34 20 20 35 20 20 36 20 20 37 20 20 38 20 20 39 20 20 31 30 20 20 31 31 20 20 31 32 20 20 31 33 20 20 31 34 20 ... 65486 more bytes>
------------
<Buffer 39 34 39 20 20 31 30 39 35 30 20 20 31 30 39 35 31 20 20 31 30 39 35 32 20 20 31 30 39 35 33 20 20 31 30 39 35 34 20 20 31 30 39 35 35 20 20 31 30 39 ... 65486 more bytes>
------------
<Buffer 31 20 20 32 30 33 31 32 20 20 32 30 33 31 33 20 20 32 30 33 31 34 20 20 32 30 33 31 35 20 20 32 30 33 31 36 20 20 32 30 33 31 37 20 20 32 30 33 31 38 ... 65486 more bytes>
------------
<Buffer 20 32 39 36 37 34 20 20 32 39 36 37 35 20 20 32 39 36 37 36 20 20 32 39 36 37 37 20 20 32 39 36 37 38 20 20 32 39 36 37 39 20 20 32 39 36 38 30 20 20 ... 65486 more bytes>
------------
<Buffer 39 30 33 36 20 20 33 39 30 33 37 20 20 33 39 30 33 38 20 20 33 39 30 33 39 20 20 33 39 30 34 30 20 20 33 39 30 34 31 20 20 33 39 30 34 32 20 20 33 39 ... 65486 more bytes>
------------
<Buffer 39 38 20 20 34 38 33 39 39 20 20 34 38 34 30 30 20 20 34 38 34 30 31 20 20 34 38 34 30 32 20 20 34 38 34 30 33 20 20 34 38 34 30 34 20 20 34 38 34 30 ... 65486 more bytes>
------------


const fs = require("node:fs/promises");

(async()=>{
    const fileHandleRead = await fs.open("test.txt", "r")

    const stream = fileHandleRead.createReadStream()

    stream.on("data", (chunk)=>{
        console.log("------------")
        console.log(chunk)
        console.log(chunk.length)
    })
})()


output ----->

------------
<Buffer 20 30 20 20 31 20 20 32 20 20 33 20 20 34 20 20 35 20 20 36 20 20 37 20 20 38 20 20 39 20 20 31 30 20 20 31 31 20 20 31 32 20 20 31 33 20 20 31 34 20 ... 65486 more bytes>
65536
------------
<Buffer 39 34 39 20 20 31 30 39 35 30 20 20 31 30 39 35 31 20 20 31 30 39 35 32 20 20 31 30 39 35 33 20 20 31 30 39 35 34 20 20 31 30 39 35 35 20 20 31 30 39 ... 65486 more bytes>
65536
------------
<Buffer 31 20 20 32 30 33 31 32 20 20 32 30 33 31 33 20 20 32 30 33 31 34 20 20 32 30 33 31 35 20 20 32 30 33 31 36 20 20 32 30 33 31 37 20 20 32 30 33 31 38 ... 65486 more bytes>
65536
------------
<Buffer 20 32 39 36 37 34 20 20 32 39 36 37 35 20 20 32 39 36 37 36 20 20 32 39 36 37 37 20 20 32 39 36 37 38 20 20 32 39 36 37 39 20 20 32 39 36 38 30 20 20 ... 65486 more bytes>
65536
------------



This way we can change the highwatermark value 

const fs = require("node:fs/promises");

(async()=>{
    const fileHandleRead = await fs.open("test.txt", "r")

    const stream = fileHandleRead.createReadStream({highWaterMark: 400})

    stream.on("data", (chunk)=>{
        console.log("------------")
        console.log(chunk.length)//Highwatermak value 400 bytes
    })
})()


Now a real stream could have three different states.
When you first create your stream, you're not reading anything.
As soon as you add the data events, what's going to happen is that the stream will start to read from
that internal resource, and then you're going to get the data in chunks.
If I remove this data event, then what's going to happen is that the stream will be in a pause state and
will no longer give me any data.
And also, I have one more event available on the stream which is stream dot on end.
And if I call that, the callback function will be executed when we are done reading that source.


In Node.js, Readable streams can be in three main states:

Idle / Not Flowing (initial state)

When you first create the stream, it’s not reading anything yet.
No data is being consumed because you haven’t attached a 'data' event listener 
or called .resume().

Flowing

Once you attach a 'data' event listener or call .resume(), the stream automatically starts reading from the internal resource.
Data flows continuously and you receive it in chunks via the 'data' event.


Paused

If you remove the 'data' listener or call .pause(), the stream goes into a paused state.
Data won’t be emitted until you explicitly resume it again.
And finally, when the stream reaches the end of its resource, 
it emits the 'end' event — meaning no more data will come,
but that’s not considered a "state" itself, just a signal.

⚡ Quick recap:

Idle / Not Flowing
Flowing
Paused




**************************************************
Splitting issue

[0, 1, 2, 3,.........................,32188]

---->
because we are getting chunks

1st chunk
[0,1,2,....32] -->In 1st chunk we are not getting 188

2nd chunk

[188, 32189,......] --->In second chunk we get the remainder

1st element of the 1st chunk is always okay and
last element of the last chunk is also completely okay


7.9 MB = 7,892,693 bytes * 8 = 63,141,544 bits (63 million bits)
1 byte = 8 bits
1 hexadecimal = 0000
utf-8 : i character is representing using 0000 0000 (8 bits)

[0,1,2] ---> Regular array

<0000 0000, 0001 0010> ---->  2byte buffer

in streams we are reading 65000 byte chunks