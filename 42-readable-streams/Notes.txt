const fs = require("node:fs/promises");

(async()=>{
    const fileHandleRead = await fs.open("test.txt", "r")

    const stream = fileHandleRead.createReadStream()

    stream.on("data", (chunk)=>{
        console.log("------------")
        console.log(chunk)
    })
})()


output ------>>>>>

------------
<Buffer 20 30 20 20 31 20 20 32 20 20 33 20 20 34 20 20 35 20 20 36 20 20 37 20 20 38 20 20 39 20 20 31 30 20 20 31 31 20 20 31 32 20 20 31 33 20 20 31 34 20 ... 65486 more bytes>
------------
<Buffer 39 34 39 20 20 31 30 39 35 30 20 20 31 30 39 35 31 20 20 31 30 39 35 32 20 20 31 30 39 35 33 20 20 31 30 39 35 34 20 20 31 30 39 35 35 20 20 31 30 39 ... 65486 more bytes>
------------
<Buffer 31 20 20 32 30 33 31 32 20 20 32 30 33 31 33 20 20 32 30 33 31 34 20 20 32 30 33 31 35 20 20 32 30 33 31 36 20 20 32 30 33 31 37 20 20 32 30 33 31 38 ... 65486 more bytes>
------------
<Buffer 20 32 39 36 37 34 20 20 32 39 36 37 35 20 20 32 39 36 37 36 20 20 32 39 36 37 37 20 20 32 39 36 37 38 20 20 32 39 36 37 39 20 20 32 39 36 38 30 20 20 ... 65486 more bytes>
------------
<Buffer 39 30 33 36 20 20 33 39 30 33 37 20 20 33 39 30 33 38 20 20 33 39 30 33 39 20 20 33 39 30 34 30 20 20 33 39 30 34 31 20 20 33 39 30 34 32 20 20 33 39 ... 65486 more bytes>
------------
<Buffer 39 38 20 20 34 38 33 39 39 20 20 34 38 34 30 30 20 20 34 38 34 30 31 20 20 34 38 34 30 32 20 20 34 38 34 30 33 20 20 34 38 34 30 34 20 20 34 38 34 30 ... 65486 more bytes>
------------


const fs = require("node:fs/promises");

(async()=>{
    const fileHandleRead = await fs.open("test.txt", "r")

    const stream = fileHandleRead.createReadStream()

    stream.on("data", (chunk)=>{
        console.log("------------")
        console.log(chunk)
        console.log(chunk.length)
    })
})()


output ----->

------------
<Buffer 20 30 20 20 31 20 20 32 20 20 33 20 20 34 20 20 35 20 20 36 20 20 37 20 20 38 20 20 39 20 20 31 30 20 20 31 31 20 20 31 32 20 20 31 33 20 20 31 34 20 ... 65486 more bytes>
65536
------------
<Buffer 39 34 39 20 20 31 30 39 35 30 20 20 31 30 39 35 31 20 20 31 30 39 35 32 20 20 31 30 39 35 33 20 20 31 30 39 35 34 20 20 31 30 39 35 35 20 20 31 30 39 ... 65486 more bytes>
65536
------------
<Buffer 31 20 20 32 30 33 31 32 20 20 32 30 33 31 33 20 20 32 30 33 31 34 20 20 32 30 33 31 35 20 20 32 30 33 31 36 20 20 32 30 33 31 37 20 20 32 30 33 31 38 ... 65486 more bytes>
65536
------------
<Buffer 20 32 39 36 37 34 20 20 32 39 36 37 35 20 20 32 39 36 37 36 20 20 32 39 36 37 37 20 20 32 39 36 37 38 20 20 32 39 36 37 39 20 20 32 39 36 38 30 20 20 ... 65486 more bytes>
65536
------------



This way we can change the highwatermark value 

const fs = require("node:fs/promises");

(async()=>{
    const fileHandleRead = await fs.open("test.txt", "r")

    const stream = fileHandleRead.createReadStream({highWaterMark: 400})

    stream.on("data", (chunk)=>{
        console.log("------------")
        console.log(chunk.length)//Highwatermak value 400 bytes
    })
})()


Now a real stream could have three different states.
When you first create your stream, you're not reading anything.
As soon as you add the data events, what's going to happen is that the stream will start to read from
that internal resource, and then you're going to get the data in chunks.
If I remove this data event, then what's going to happen is that the stream will be in a pause state and
will no longer give me any data.
And also, I have one more event available on the stream which is stream dot on end.
And if I call that, the callback function will be executed when we are done reading that source.


In Node.js, Readable streams can be in three main states:

Idle / Not Flowing (initial state)

When you first create the stream, it’s not reading anything yet.
No data is being consumed because you haven’t attached a 'data' event listener 
or called .resume().

Flowing

Once you attach a 'data' event listener or call .resume(), the stream automatically starts reading from the internal resource.
Data flows continuously and you receive it in chunks via the 'data' event.


Paused

If you remove the 'data' listener or call .pause(), the stream goes into a paused state.
Data won’t be emitted until you explicitly resume it again.
And finally, when the stream reaches the end of its resource, 
it emits the 'end' event — meaning no more data will come,
but that’s not considered a "state" itself, just a signal.

⚡ Quick recap:

Idle / Not Flowing
Flowing
Paused




**************************************************
Splitting issue

[0, 1, 2, 3,.........................,32188]

---->
because we are getting chunks

1st chunk
[0,1,2,....32] -->In 1st chunk we are not getting 188

2nd chunk

[188, 32189,......] --->In second chunk we get the remainder

1st element of the 1st chunk is always okay and
last element of the last chunk is also completely okay


7.9 MB = 7,892,693 bytes * 8 = 63,141,544 bits (63 million bits)
1 byte = 8 bits
1 hexadecimal = 0000
utf-8 : i character is representing using 0000 0000 (8 bits)

[0,1,2] ---> Regular array

<0000 0000, 0001 0010> ---->  2byte buffer

in streams we are reading 65000 byte chunks

*************************************************************

what happens if streamRead.pause() triggers in the middle of a chunk.

✅ What your code does:

Reads bigFile.txt using a stream with highWaterMark: 64KB.
Splits the chunk into numbers by " " (double space).
Repairs edge cases when a number is split across two chunks.
Writes only even numbers to destBig.txt.
If streamWrite.write() returns false (buffer full), you call streamRead.pause().
On drain, you resume reading.

⚠️ The subtle issue

If you call:

if (!streamWrite.write(" " + n + " ")) {
  streamRead.pause();
}


inside the numbers.forEach loop, then you may pause reading in the middle of processing a chunk.
That means:
Some numbers from this chunk before pause will be written.

The rest of the numbers in this chunk (after the pause) will be skipped entirely because your 
on("data") handler won’t re-run for the “half-processed” chunk.

The stream does not replay the same chunk after resuming — it just continues with the next chunk.
So those remaining numbers are lost.

🔧 How to fix it

You need to buffer the rest of the numbers when a pause happens, and resume processing them when drain fires.
Example adjustment:

let pendingNumbers = [];

streamRead.on("data", (chunk) => {
  const numbers = chunk.toString("utf-8").split("  ");
  // handle split edge cases like you already do...
  
  numbers.forEach((number) => {
    let n = Number(number);

    if (n % 2 === 0) {
      if (!streamWrite.write(" " + n + " ")) {
        streamRead.pause();
        // Save remaining numbers to process later
        pendingNumbers = numbers.slice(numbers.indexOf(number) + 1);
        return; // exit early, wait for drain
      }
    }
  });
});

streamWrite.on("drain", () => {
  // process remaining buffered numbers
  while (pendingNumbers.length) {
    const number = pendingNumbers.shift();
    let n = Number(number);

    if (n % 2 === 0) {
      if (!streamWrite.write(" " + n + " ")) {
        // still full, keep leftovers and wait for next drain
        return;
      }
    }
  }
  // resume reading once backlog is cleared
  streamRead.resume();
});

⚡ Summary

Problem: If pause() happens mid-chunk, the remaining numbers in that chunk get skipped.
Fix: Store the leftover numbers and flush them on drain before resuming the read stream.


***************************************************************

const fs = require("node:fs/promises");

(async () => {
  console.time("readBig");

  const fileHandleRead = await fs.open("bigFile.txt", "r");
  const fileHandleWrite = await fs.open("destBig.txt", "w");

  const streamRead = fileHandleRead.createReadStream({
    highWaterMark: 64 * 1024, // 64KB chunks
  });
  const streamWrite = fileHandleWrite.createWriteStream();

  let leftover = "";            // store split number between chunks
  let pendingNumbers = [];      // backlog when backpressure occurs
  let pendingIndex = 0;         // pointer into pendingNumbers

  function processNumbers(numbers) {
    for (; pendingIndex < numbers.length; pendingIndex++) {
      let n = Number(numbers[pendingIndex]);

      if (!isNaN(n) && n % 2 === 0) {
        if (!streamWrite.write(" " + n + " ")) {
          // Buffer full → pause read + keep backlog
          streamRead.pause();
          pendingNumbers = numbers;
          return; // stop early, wait for 'drain'
        }
      }
    }
    // all numbers processed, reset backlog
    pendingNumbers = [];
    pendingIndex = 0;
  }

  streamRead.on("data", (chunk) => {
    // prepend leftover (from previous chunk) before splitting
    const numbers = (leftover + chunk.toString("utf-8")).split("  ");

    // handle chunk end split (last number may be incomplete)
    leftover = numbers.pop();

    // process the numbers safely
    processNumbers(numbers);
  });

  streamWrite.on("drain", () => {
    // continue backlog first
    if (pendingNumbers.length > 0) {
      processNumbers(pendingNumbers);
    }
    // if backlog cleared, resume reading
    if (pendingNumbers.length === 0) {
      streamRead.resume();
    }
  });

  streamRead.on("end", () => {
    // handle any leftover number at the very end
    if (leftover) {
      let n = Number(leftover.trim());
      if (!isNaN(n) && n % 2 === 0) {
        streamWrite.write(" " + n + " ");
      }
    }

    console.log("Done reading");
    console.timeEnd("readBig");
  });
})();
